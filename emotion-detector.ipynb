{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47ca04ad-58e3-48f7-a652-bac9d6577344",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, Dropout,  GlobalAveragePooling2D\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator, load_img, img_to_array\n",
    "from tensorflow.keras.preprocessing import image\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from tensorflow.keras.applications import VGG16, ResNet50, MobileNetV2, EfficientNetV2B0\n",
    "from tensorflow.keras.applications.efficientnet_v2 import preprocess_input\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint, ReduceLROnPlateau\n",
    "import numpy as np\n",
    "import os\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "794fad55-5c22-4012-8b16-e1afbb9413ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ! pip install kagglehub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5185cc2-f49d-4d08-8e54-17bd40b39e1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ! pip install streamlit"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4bb79cfd-3051-4fe6-88c1-5969035da18a",
   "metadata": {},
   "source": [
    "## Downloading Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c81266fe-4726-45dc-b431-ed23ddf3e86f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import kagglehub\n",
    "\n",
    "# # Download latest version\n",
    "# path = kagglehub.dataset_download(\"jonathanoheix/face-expression-recognition-dataset\")\n",
    "\n",
    "# print(\"Path to dataset files:\", path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a060f060-5f76-4d71-bb75-3b494058b583",
   "metadata": {},
   "outputs": [],
   "source": [
    "BASE_DIR = Path('images')  # Update this to your data directory\n",
    "TRAIN_DIR = BASE_DIR / 'train'\n",
    "VALID_DIR = BASE_DIR / 'validation'\n",
    "SAMPLE_DIR = BASE_DIR / 'sample'\n",
    "\n",
    "# Image parameters\n",
    "IMG_HEIGHT = 224\n",
    "IMG_WIDTH = 224\n",
    "IMG_CHANNELS = 3\n",
    "BATCH_SIZE = 32\n",
    "\n",
    "print(f\"Train directory exists: {TRAIN_DIR.exists()}\")\n",
    "print(f\"Valid directory exists: {VALID_DIR.exists()}\")\n",
    "print(f\"Sample directory exists: {SAMPLE_DIR.exists()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e022b16f-3e72-449e-a962-20746e1adfe7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Count images in each set\n",
    "def count_images(directory):\n",
    "    \"\"\"Count total images and images per class in a directory.\"\"\"\n",
    "    total = 0\n",
    "    class_counts = {}\n",
    "    \n",
    "    for class_dir in sorted(directory.iterdir()):\n",
    "        if class_dir.is_dir():\n",
    "            num_images = len(list(class_dir.glob('*.jpg')))\n",
    "            class_counts[class_dir.name] = num_images\n",
    "            total += num_images\n",
    "    \n",
    "    return total, class_counts\n",
    "\n",
    "# Count images in each set\n",
    "train_total, train_counts = count_images(TRAIN_DIR)\n",
    "valid_total, valid_counts = count_images(VALID_DIR)\n",
    "sample_total = len(os.listdir(SAMPLE_DIR))\n",
    "\n",
    "print(f\"Training images: {train_total}\")\n",
    "print(f\"Validation images: {valid_total}\")\n",
    "print(f\"Sample images: {sample_total}\")\n",
    "print(f\"\\nNumber of classes (emotions): {len(train_counts)}\")\n",
    "print(f\"\\nAll 7 Emotions: {list(train_counts.keys())[:10]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a12607a2-abbb-4d2d-a994-3d0292abbf58",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_happy = len(os.listdir(os.path.join(TRAIN_DIR, 'happy')))\n",
    "train_sad = len(os.listdir(os.path.join(TRAIN_DIR, 'sad')))\n",
    "train_fear = len(os.listdir(os.path.join(TRAIN_DIR, 'fear')))\n",
    "train_disgust = len(os.listdir(os.path.join(TRAIN_DIR, 'disgust')))\n",
    "train_angry = len(os.listdir(os.path.join(TRAIN_DIR, 'angry')))\n",
    "train_neutral = len(os.listdir(os.path.join(TRAIN_DIR, 'neutral')))\n",
    "train_surprise = len(os.listdir(os.path.join(TRAIN_DIR, 'surprise')))\n",
    "\n",
    "print(f\"Training happy: {train_happy} images\")\n",
    "print(f\"Training sad: {train_sad} images\")\n",
    "print(f\"Training fear: {train_fear} images\")\n",
    "print(f\"Training disgust: {train_disgust} images\")\n",
    "print(f\"Training angry: {train_angry} images\")\n",
    "print(f\"Training neutral: {train_neutral} images\")\n",
    "print(f\"Training surprise: {train_surprise} images\")\n",
    "print(f\"Total training images: {train_happy + train_sad + train_fear + train_disgust + train_angry + train_neutral + train_surprise}\")\n",
    "# print(f\"Total training images: {count_images(TRAIN_DIR)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c54c91c7-66bc-49ab-867f-2bf4a907d179",
   "metadata": {},
   "outputs": [],
   "source": [
    "valid_happy = len(os.listdir(os.path.join(VALID_DIR, 'happy')))\n",
    "valid_sad = len(os.listdir(os.path.join(VALID_DIR, 'sad')))\n",
    "valid_fear = len(os.listdir(os.path.join(VALID_DIR, 'fear')))\n",
    "valid_disgust = len(os.listdir(os.path.join(VALID_DIR, 'disgust')))\n",
    "valid_angry = len(os.listdir(os.path.join(VALID_DIR, 'angry')))\n",
    "valid_neutral = len(os.listdir(os.path.join(VALID_DIR, 'neutral')))\n",
    "valid_surprise = len(os.listdir(os.path.join(VALID_DIR, 'surprise')))\n",
    "\n",
    "print(f\"Validation happy: {valid_happy} images\")\n",
    "print(f\"Validation sad: {valid_sad} images\")\n",
    "print(f\"Validation fear: {valid_fear} images\")\n",
    "print(f\"Validation disgust: {valid_disgust} images\")\n",
    "print(f\"Validation angry: {valid_angry} images\")\n",
    "print(f\"Validation neutral: {valid_neutral} images\")\n",
    "print(f\"Validation surprise: {valid_surprise} images\")\n",
    "print(f\"Total validation images: {valid_happy + valid_sad + valid_fear + valid_disgust + valid_angry + valid_neutral + valid_surprise}\")\n",
    "# print(f\"Total validing images: {count_images(TRAIN_DIR)}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3077a96-1e33-4096-a646-3ce65e3f069f",
   "metadata": {},
   "source": [
    "## Visualisations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9f91fc4-2dd7-4395-8e5c-0bb334d0a5eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(list(train_counts.items()), columns=['Class', 'Count'])\n",
    "df.sort_values(by='Count', ascending=False).head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f1a9c41-9d34-4aba-b48d-ce8bde918d96",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f724992-1053-4d7d-a1a7-34f3f81692e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize class distribution\n",
    "def plot_class_distribution(class_counts, title=\"Class Distribution\", n=7):\n",
    "    \"\"\"Plot class distribution as a bar chart.\"\"\"\n",
    "    df = pd.DataFrame(list(class_counts.items()), columns=['Class', 'Count'])\n",
    "    df = df.sort_values('Count', ascending=False).head(n)\n",
    "    \n",
    "    plt.figure(figsize=(14, 6))\n",
    "    plt.bar(range(len(df)), df['Count'], color='steelblue')\n",
    "    plt.xlabel('Emotion', fontsize=12)\n",
    "    plt.ylabel('Number of Images', fontsize=12)\n",
    "    plt.title(f'{title} ({n})', fontsize=14)\n",
    "    plt.xticks(range(len(df)), df['Class'], rotation=90, ha='right', fontsize=8)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    print(f\"Mean images per class: {df['Count'].mean():.2f}\")\n",
    "    print(f\"Std images per class: {df['Count'].std():.2f}\")\n",
    "    print(f\"Min images per class: {df['Count'].min()}\")\n",
    "    print(f\"Max images per class: {df['Count'].max()}\")\n",
    "\n",
    "plot_class_distribution(train_counts, \"Training Set Class Distribution\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a9949e7-f211-4a9c-ab68-ee1c8612981a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ! pip install plotly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce3bb96f-6ad0-4c6d-8035-6d1b71c5a5a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.groupby('Class')['Count'].sum().sort_values(ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48c9aa90-4bc3-4e7f-ad74-4e38d92e9915",
   "metadata": {},
   "outputs": [],
   "source": [
    "Class_Id_Dist_Total = df.groupby('Class')['Count'].sum().sort_values(ascending=False)\n",
    "# Class_Id_Dist_Total = df['Class'].value_counts(sort=False)\n",
    "\n",
    "\n",
    "import plotly.express as px\n",
    "fig=px.pie(Class_Id_Dist_Total,values= Class_Id_Dist_Total.values, names=Class_Id_Dist_Total.index,hole=0.500)\n",
    "fig.update_layout(title='Data Distribution of Emotions Dataset',font_size=15,title_x=0.45,annotations=[dict(text='Emotions Dataset',font_size=12, showarrow=False,height=1000,width=1000)])\n",
    "fig.update_traces(textfont_size=15,textinfo='percent')\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee0416a3-573a-45b1-b988-3bd74af4151d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_sample_images(directory, num_samples=12, figsize=(15, 10)):\n",
    "    \"\"\"Plot random sample images from the dataset.\"\"\"\n",
    "    classes = sorted([d.name for d in directory.iterdir() if d.is_dir()])\n",
    "    selected_classes = np.random.choice(classes, min(num_samples, len(classes)), replace=False)\n",
    "    \n",
    "    fig, axes = plt.subplots(2, 4, figsize=figsize)\n",
    "    axes = axes.ravel()\n",
    "    \n",
    "    for idx, class_name in enumerate(selected_classes):\n",
    "        class_dir = directory / class_name\n",
    "        images = list(class_dir.glob('*.jpg'))\n",
    "        \n",
    "        if images:\n",
    "            random_image = np.random.choice(images)\n",
    "            img = load_img(random_image, target_size=(IMG_HEIGHT, IMG_WIDTH))\n",
    "            \n",
    "            axes[idx].imshow(img)\n",
    "            axes[idx].set_title(class_name, fontsize=10)\n",
    "            axes[idx].axis('off')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.suptitle('Sample Butterfly Images', fontsize=16, y=1.02)\n",
    "    plt.show()\n",
    "\n",
    "plot_sample_images(TRAIN_DIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51257e42-9761-4e54-a636-203f4bee3e35",
   "metadata": {},
   "outputs": [],
   "source": [
    "# sns.heatmap(df, cmap='coolwarm')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29721efb-5056-49ea-b679-479d2686fa99",
   "metadata": {},
   "outputs": [],
   "source": [
    "happy_image_path = os.path.join(SAMPLE_DIR, 'happy_img.webp')\n",
    "\n",
    "print(f\"Loading image from: {happy_image_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4700b24f-0ad1-4634-96c1-f11b2f4ed47f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Open the image using PIL\n",
    "happy_image = Image.open(happy_image_path)\n",
    "\n",
    "print(f\"Image loaded successfully!\")\n",
    "print(f\"Image format: {happy_image.format}\")\n",
    "print(f\"Image mode: {happy_image.mode}\")\n",
    "print(f\"Image size: {happy_image.size}\")\n",
    "\n",
    "plt.imshow(happy_image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24bf432a-dfa2-4a7b-b3c8-fe8f1435f842",
   "metadata": {},
   "outputs": [],
   "source": [
    "happy_array = np.array(happy_image)\n",
    "\n",
    "red_channel = happy_array[:, :, 0]    # First channel (index 0)\n",
    "green_channel = happy_array[:, :, 1]  # Second channel (index 1)\n",
    "blue_channel = happy_array[:, :, 2]   # Third channel (index 2)\n",
    "\n",
    "print(f\"Red channel shape: {red_channel.shape}\")\n",
    "print(f\"Green channel shape: {green_channel.shape}\")\n",
    "print(f\"Blue channel shape: {blue_channel.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b42b5ea7-084e-44c1-940a-2b048747afc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display all three channels side by side\n",
    "fig, axes = plt.subplots(1, 4, figsize=(16, 4))\n",
    "\n",
    "# Original image\n",
    "axes[0].imshow(happy_image)\n",
    "axes[0].set_title('Original Image')\n",
    "axes[0].axis('off')\n",
    "\n",
    "# Red channel\n",
    "axes[1].imshow(red_channel, cmap='Reds')\n",
    "axes[1].set_title('Red Channel')\n",
    "axes[1].axis('off')\n",
    "\n",
    "# Green channel\n",
    "axes[2].imshow(green_channel, cmap='Greens')\n",
    "axes[2].set_title('Green Channel')\n",
    "axes[2].axis('off')\n",
    "\n",
    "# Blue channel\n",
    "axes[3].imshow(blue_channel, cmap='Blues')\n",
    "axes[3].set_title('Blue Channel')\n",
    "axes[3].axis('off')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d807324e-c683-4263-80d2-504f38b46fa1",
   "metadata": {},
   "outputs": [],
   "source": [
    "small_patch = happy_array[50:55, 50:55, 0]  # 5x5 patch from red channel\n",
    "\n",
    "print(\"Pixel values from a 5x5 patch (Red channel):\")\n",
    "print(small_patch)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8cd2fde6-dd00-42f2-bfbd-5187287bda97",
   "metadata": {},
   "source": [
    "## Data Preprocessing and Augmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4840dcd7-cd4a-4d16-acb1-664443deac02",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = ImageDataGenerator(\n",
    "    # rescale=1./255, # normalize pixel values to 0-1\n",
    "    rotation_range=40, # rotating images up to 40 degress \n",
    "    width_shift_range=0.2, # randomly shifting horizontally by 20% \n",
    "    height_shift_range=0.2, # randomly shifting vertically by 20% \n",
    "    shear_range=0.2, \n",
    "    zoom_range=0.5,\n",
    "    horizontal_flip=True,\n",
    "    fill_mode='nearest',\n",
    "    preprocessing_function=preprocess_input\n",
    ")\n",
    "train_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "619a06fc-2971-4047-ab36-aeb5571be4a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "val_data = ImageDataGenerator(\n",
    "    preprocessing_function=preprocess_input\n",
    ")\n",
    "val_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05f8e8d7-ceed-41c7-a5df-eb11f1082629",
   "metadata": {},
   "outputs": [],
   "source": [
    "target_size = (224,224) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5c7472a-32e6-47bc-8a4e-0b388e44cbaf",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_gen = train_data.flow_from_directory(\n",
    "    TRAIN_DIR,\n",
    "    target_size=target_size,\n",
    "    batch_size=32, \n",
    "    class_mode='categorical',\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d672e572-fd4c-4e1a-8144-074074f0afd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "val_gen = val_data.flow_from_directory(\n",
    "    VALID_DIR,\n",
    "    target_size=target_size,\n",
    "    batch_size=32, \n",
    "    class_mode='categorical',\n",
    "    shuffle=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7081044-baab-4938-b38c-26fe1b167e8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_gen.class_indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ef94275-efcb-420e-afb1-c6b1814927c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "early_stopping = EarlyStopping(\n",
    "    monitor='val_loss',\n",
    "    patience=5, \n",
    "    restore_best_weights=True\n",
    ") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce5acde4-f031-479f-8d81-2efe498b7fa2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "import numpy as np\n",
    "\n",
    "class_names = list(val_gen.class_indices.keys())\n",
    "class_weights = compute_class_weight(\n",
    "    class_weight=\"balanced\",\n",
    "    classes=np.arange(len(class_names)),\n",
    "    y=train_gen.classes\n",
    ")\n",
    "\n",
    "class_weights = dict(enumerate(class_weights))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "453862ac-9b87-47c8-9e6f-1e07ea40e48d",
   "metadata": {},
   "outputs": [],
   "source": [
    "base_model = MobileNetV2(\n",
    "    include_top=False,\n",
    "    weights='imagenet',\n",
    "    input_shape= (224,224,3)\n",
    ")\n",
    "\n",
    "base_model.trainable=False\n",
    "\n",
    "model = Sequential([\n",
    "    base_model,\n",
    "    GlobalAveragePooling2D(),\n",
    "    Dense(256,activation='relu'),\n",
    "    Dropout(0.5),\n",
    "    Dense(128,activation='relu'),\n",
    "    Dropout(0.5),\n",
    "    Dense(7,activation='softmax')\n",
    "])\n",
    "\n",
    "model.compile(optimizer='adam', loss='categorical_crossentropy',metrics=['accuracy'])\n",
    "model.summary()\n",
    "\n",
    "# history = model.fit(\n",
    "#     train_gen,\n",
    "#     validation_data = val_gen,\n",
    "#     epochs=10,\n",
    "#     callbacks=[early_stopping]\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b5910c7-e2db-46f8-ab1b-ab00b62d302b",
   "metadata": {},
   "outputs": [],
   "source": [
    "history = model.fit(\n",
    "    train_gen,\n",
    "    validation_data=val_gen,\n",
    "    epochs=10,\n",
    "    callbacks=[early_stopping],\n",
    "    class_weight=class_weights\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18c2ef28-c4a2-4b39-b036-63edc768189f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # [AI-Generated]\n",
    "# base_model.trainable = True\n",
    "\n",
    "# for layer in base_model.layers[:-40]:\n",
    "#     layer.trainable = False\n",
    "\n",
    "# model.compile(\n",
    "#     optimizer=tf.keras.optimizers.Adam(learning_rate=1e-4),\n",
    "#     loss='categorical_crossentropy',\n",
    "#     metrics=['accuracy']\n",
    "# )\n",
    "\n",
    "# history2 = model.fit(\n",
    "#     train_gen,\n",
    "#     validation_data=val_gen,\n",
    "#     epochs=20,\n",
    "#     callbacks=[early_stopping]\n",
    "# )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66c423e5-545e-4725-bab5-3f2c871883d7",
   "metadata": {},
   "source": [
    "#### Visualisation of data augmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c2d342c-0c54-4e3b-82fc-9e24783568e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_path_img = os.path.join(SAMPLE_DIR, 'happy_img.webp')\n",
    "sample_img = image.load_img(sample_path_img, target_size=target_size)\n",
    "sample_img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a212feae-7586-49de-9cb9-1c0d33625014",
   "metadata": {},
   "outputs": [],
   "source": [
    "arr = image.img_to_array(sample_img)\n",
    "arr_img = arr.reshape((1,)+ arr.shape).shape\n",
    "arr_img"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8adaead3-56ec-482c-803b-e8185e4d08ed",
   "metadata": {},
   "source": [
    "### Training curves"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8080324e-b768-4bff-9471-91129a0aa21d",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(history.history).plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd219e4e-c690-45be-ad3f-11f206b8c580",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(history.history['val_loss'],label='validation loss')\n",
    "plt.plot(history.history['loss'],label='training loss')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46ea607f-f76b-4661-9f3e-fb485e2de006",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(history.history['accuracy'],label='training accuracy')\n",
    "plt.plot(history.history['val_accuracy'],label='validation accuracy')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "839f4d4d-bf4e-40dd-a23d-cc414034ed7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save('emotion.keras')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5de2a6c1-efe4-45d3-805b-9978123a8998",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras import models\n",
    "model = models.load_model('emotion.keras')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50991ed7-e04a-4c08-a3c0-11c804c944c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from tensorflow.keras.preprocessing.image import load_img, img_to_array\n",
    "from PIL import Image\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57e89185-d7c9-4d39-8373-baf74c031087",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_images = [f for f in os.listdir(SAMPLE_DIR) if f.endswith(('.jpg','.jpeg','.png','.webp'))]\n",
    "\n",
    "fig,axes = plt.subplots(2,4,figsize=(16,8))\n",
    "axes = axes.flatten()\n",
    "\n",
    "for i, img_name in enumerate(sample_images):\n",
    "    img_path = os.path.join(SAMPLE_DIR,img_name)\n",
    "    img = image.load_img(img_path, target_size=(224,224))\n",
    "    img_array = image.img_to_array(img)\n",
    "    img_array = np.expand_dims(img_array,axis=0)\n",
    "    img_array = img_array/255.0\n",
    "\n",
    "    pred  = model.predict(img_array)\n",
    "\n",
    "    class_labels = ['angry',\n",
    " 'disgust',\n",
    " 'fear',\n",
    " 'happy',\n",
    " 'neutral',\n",
    " 'sad',\n",
    " 'surprise']\n",
    "\n",
    "    # [AI-Generated]\n",
    "    class_index = np.argmax(pred[0])\n",
    "    conf = pred[0][class_index] * 100\n",
    "    label = class_labels[class_index]\n",
    "\n",
    "    axes[i].imshow(img)\n",
    "    axes[i].set_title(f'{label}\\n({conf:.1f}%)confident')\n",
    "    axes[i].axis('off')\n",
    "plt.suptitle('Model Prediction on sample images')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32142f19-6a7b-4d9c-88de-83681e6496d5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "daeb6c70-099f-4f5c-8105-aec5accf67a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "pred = model.predict(img_array)\n",
    "print(pred)\n",
    "print(\"Sum:\", pred[0].sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25a760ed-fd80-4141-8abb-aa5d0faaf5e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# map index â†’ class name\n",
    "class_labels = ['angry',\n",
    " 'disgust',\n",
    " 'fear',\n",
    " 'happy',\n",
    " 'neutral',\n",
    " 'sad',\n",
    " 'surprise']\n",
    "\n",
    "sample_images = [f for f in os.listdir(SAMPLE_DIR) if f.endswith(('.jpg','.jpeg','.png','.webp'))]\n",
    "\n",
    "fig,axes = plt.subplots(2,4,figsize=(16,8))\n",
    "axes = axes.flatten()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "for i, img_name in enumerate(sample_images):\n",
    "    img_path = os.path.join(SAMPLE_DIR,img_name)\n",
    "    img = load_img(img_path, target_size=(150,150))\n",
    "    img_array = img_to_array(img)\n",
    "    img_array = np.expand_dims(img_array,axis=0)\n",
    "    img_array = img_array/255.0\n",
    "\n",
    "    pred  = model.predict(img_array)\n",
    "\n",
    "    class_index = np.argmax(pred[0])\n",
    "    conf = pred[0][class_index] * 100\n",
    "    label = class_labels[class_index]\n",
    "    \n",
    "    axes[i].imshow(img)\n",
    "    axes[i].set_title(f'{label}\\n({conf:.1f}%)confident')\n",
    "    axes[i].axis('off')\n",
    "plt.suptitle('Model Prediction on sample images')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8dc5b17-d820-4c2e-8d47-a5b3e2cc053b",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8213b7c3-f170-4047-b85d-0d40c6c8ecce",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = model.predict(val_gen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46664692-2574-4fb5-b2f8-df65dd3df8e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import ConfusionMatrixDisplay, classification_report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dea30ac4-cb7f-450c-8280-a8f8029a5446",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_true = val_gen.classes\n",
    "y_true"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4984bdd-2912-4cbc-924a-7410e2cdb244",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred2 = np.argmax(y_pred, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8aa304c3-e92e-4e9c-9f36-02d6d3d1394b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "print(\"Classification Report:\\n\")\n",
    "print(classification_report(\n",
    "    y_true,\n",
    "    y_pred2,\n",
    "    target_names=class_names,\n",
    "    digits=4\n",
    "))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e542b7ba-9b76-48c2-b35b-380bee17ebda",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.metrics import ConfusionMatrixDisplay\n",
    "\n",
    "# y_pred: model.predict(val_gen), shape = (num_samples, 7)\n",
    "pred_classes = np.argmax(y_pred, axis=1)\n",
    "\n",
    "ConfusionMatrixDisplay.from_predictions(val_gen.classes,pred_classes,display_labels=['angry',\n",
    " 'disgust',\n",
    " 'fear',\n",
    " 'happy',\n",
    " 'neutral',\n",
    " 'sad',\n",
    " 'surprise'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42292652-dc5c-41e7-a67e-b74fa3bfe078",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
